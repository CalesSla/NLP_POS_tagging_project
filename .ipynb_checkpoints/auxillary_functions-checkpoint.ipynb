{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51c18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def model2png(model, filename=\"\", overwrite=False, show_ends=False):\n",
    "    nodes = model.graph.nodes()\n",
    "    if not show_ends:\n",
    "        nodes = [n for n in nodes if n not in (model.start, model.end)]\n",
    "    g = nx.relabel_nodes(model.graph.subgraph(nodes), {n: n.name for n in model.graph.nodes()})\n",
    "    pydot_graph = nx.drawing.nx_pydot.to_pydot(g)\n",
    "    pydot_graph.set_rankdir(\"LR\")\n",
    "    png_data = pydot_graph.create_png(prog='dot')\n",
    "    img_data = BytesIO()\n",
    "    img_data.write(png_data)\n",
    "    img_data.seek(0)\n",
    "    if filename:\n",
    "        if os.path.exists(filename) and not overwrite:\n",
    "            raise IOError(\"File already exists. Use overwrite=True to replace existing files on disk.\")\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(img_data.read())\n",
    "        img_data.seek(0)\n",
    "    return mplimg.imread(img_data)\n",
    "\n",
    "def show_model(model, figsize=(5, 5), **kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(model2png(model, **kwargs))\n",
    "    plt.axis('off')\n",
    "    \n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "def read_tags(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "  \n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dd938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
