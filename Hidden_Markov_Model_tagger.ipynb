{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d542f7f7",
   "metadata": {},
   "source": [
    "# Hidden Markov Model tagger project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39151d87",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea746e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%run auxillary_functions.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "from collections import defaultdict\n",
    "\n",
    "print_examples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3c49e",
   "metadata": {},
   "source": [
    "### Load dataset and print the main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1517c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset('tags-universal.txt', 'brown-universal.txt', train_test_split = 0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9678c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "# Sentence example\n",
    "if print_examples:\n",
    "  key = 'b100-38532'\n",
    "  print(\"Sentence: {}\".format(key))\n",
    "  print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "  print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1fffe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "# Counting unique Elements in the dataset\n",
    "if print_examples:\n",
    "  print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "        .format(data.N, len(data.vocab)))\n",
    "  print(\"There are {} samples of {} unique words in the training set.\"\n",
    "        .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "  print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "        .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "  print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "        .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "  assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "         \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "26f1970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing word and tag sequences\n",
    "if print_examples:\n",
    "  for i in range(2):    \n",
    "      print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "      print()\n",
    "      print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9caae449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n"
     ]
    }
   ],
   "source": [
    "# Accessing (word, tag) samples\n",
    "\n",
    "if print_examples:\n",
    "  print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "  for i, pair in enumerate(data.stream()):\n",
    "      print(\"\\t\", pair)\n",
    "      if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44582492",
   "metadata": {},
   "source": [
    "### Pair counts implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c2c42",
   "metadata": {},
   "source": [
    "#### Implementation 1 - suboptimal. Dictionary and conditional statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f867cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = {}\n",
    "\n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       if pair[1] not in pair_counts.keys():\n",
    "#         pair_counts[pair[1]] = {}\n",
    "#         if pair[0] not in pair_counts[pair[1]].keys():\n",
    "#           pair_counts[pair[1]][pair[0]] = 1\n",
    "#         else:\n",
    "#           pair_counts[pair[1]][pair[0]] += 1\n",
    "\n",
    "#       else:\n",
    "#         if pair[0] not in pair_counts[pair[1]].keys():\n",
    "#           pair_counts[pair[1]][pair[0]] = 1\n",
    "#         else:\n",
    "#           pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "# assert len(emission_counts) == 12, \\\n",
    "#        \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "# assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "#        \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "# HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3b1d1",
   "metadata": {},
   "source": [
    "#### Implementation 2 - mediocre. dict and defaultdict combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "34db3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = {}\n",
    "\n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       if pair[1] not in pair_counts.keys():\n",
    "#         pair_counts[pair[1]] = defaultdict(lambda: 0)\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "#       else:\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "# assert len(emission_counts) == 12, \\\n",
    "#        \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "# assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "#        \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "# HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa9d62",
   "metadata": {},
   "source": [
    "#### Implementation 3 - optimal. Using defaultdict class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a5be5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "# assert len(emission_counts) == 12, \\\n",
    "#        \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "# assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "#        \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "# HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d55df",
   "metadata": {},
   "source": [
    "#### Implementation 4 - expected (manual). Using sequences of arbitrary lengths and a nested for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "83f41aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "  \n",
    "#     pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "#     for i in range(len(data.X)):\n",
    "#       for pair in zip(data.X[i], data.Y[i]):\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "# assert len(emission_counts) == 12, \\\n",
    "#        \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "# assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "#        \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "# HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8905dd",
   "metadata": {},
   "source": [
    "#### Implementation 5 - expected (itertools chain). Using sequences of arbitrary lengths and itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3a13bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "  \n",
    "    pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for pair in zip(tuple(chain.from_iterable(data.X)), tuple(chain.from_iterable(data.Y))):\n",
    "      pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "assert len(emission_counts) == 12, \\\n",
    "       \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "       \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add6e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde08c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf3a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dd8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501329b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155869f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f9bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681bdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d5997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df17eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
