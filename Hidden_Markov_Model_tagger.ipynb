{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc430045",
   "metadata": {},
   "source": [
    "# Hidden Markov Model tagger project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b1863",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%run auxillary_functions.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "\n",
    "print_examples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d61fc",
   "metadata": {},
   "source": [
    "### Load dataset and print the main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f349e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset('tags-universal.txt', 'brown-universal.txt', train_test_split = 0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373ad0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "# Sentence example\n",
    "if print_examples:\n",
    "  key = 'b100-38532'\n",
    "  print(\"Sentence: {}\".format(key))\n",
    "  print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "  print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9331d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "# Counting unique Elements in the dataset\n",
    "if print_examples:\n",
    "  print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "        .format(data.N, len(data.vocab)))\n",
    "  print(\"There are {} samples of {} unique words in the training set.\"\n",
    "        .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "  print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "        .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "  print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "        .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "  assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "         \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008b9452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing word and tag sequences\n",
    "if print_examples:\n",
    "  for i in range(2):    \n",
    "      print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "      print()\n",
    "      print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ddf5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n"
     ]
    }
   ],
   "source": [
    "# Accessing (word, tag) samples\n",
    "\n",
    "if print_examples:\n",
    "  print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "  for i, pair in enumerate(data.stream()):\n",
    "      print(\"\\t\", pair)\n",
    "      if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c23c42",
   "metadata": {},
   "source": [
    "### Pair counts implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d7847",
   "metadata": {},
   "source": [
    "#### Implementation 1 - suboptimal. Dictionary and conditional statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c133020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = {}\n",
    "\n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       if pair[1] not in pair_counts.keys():\n",
    "#         pair_counts[pair[1]] = {}\n",
    "#         if pair[0] not in pair_counts[pair[1]].keys():\n",
    "#           pair_counts[pair[1]][pair[0]] = 1\n",
    "#         else:\n",
    "#           pair_counts[pair[1]][pair[0]] += 1\n",
    "\n",
    "#       else:\n",
    "#         if pair[0] not in pair_counts[pair[1]].keys():\n",
    "#           pair_counts[pair[1]][pair[0]] = 1\n",
    "#         else:\n",
    "#           pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d9a21",
   "metadata": {},
   "source": [
    "#### Implementation 2 - mediocre. dict and defaultdict combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfebfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = {}\n",
    "\n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       if pair[1] not in pair_counts.keys():\n",
    "#         pair_counts[pair[1]] = defaultdict(lambda: 0)\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "#       else:\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd786247",
   "metadata": {},
   "source": [
    "#### Implementation 3 - optimal. Using defaultdict class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a30fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "#     pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "#     for i, pair in enumerate(data.stream()):\n",
    "#       pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342f881",
   "metadata": {},
   "source": [
    "#### Implementation 4 - expected (manual). Using sequences of arbitrary lengths and a nested for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6ea2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pair_counts(sequences_A, sequences_B):\n",
    "  \n",
    "#     pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "#     for i in range(len(data.X)):\n",
    "#       for pair in zip(data.X[i], data.Y[i]):\n",
    "#         pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "#     return pair_counts\n",
    "\n",
    "\n",
    "# emission_counts = pair_counts(data.tagset, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2eddf",
   "metadata": {},
   "source": [
    "#### Implementation 5 - expected (itertools chain). Using sequences of arbitrary lengths and itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dec7cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "  \n",
    "    pair_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for pair in zip(tuple(chain.from_iterable(sequences_A)), tuple(chain.from_iterable(sequences_B))):\n",
    "      pair_counts[pair[1]][pair[0]] += 1\n",
    "    \n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "emission_counts = pair_counts(data.tagset, data.vocab)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate C(t_i, w_i)\n",
    "emission_counts = pair_counts(data.training_set.X, data.training_set.Y)\n",
    "\n",
    "assert len(emission_counts) == 12, \\\n",
    "       \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "       \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b342f9",
   "metadata": {},
   "source": [
    "### Most frequent class tagger (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7024db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\n",
    "word_counts = pair_counts(data.training_set.Y, data.training_set.X)\n",
    "\n",
    "mfc_table = {k:max(v, key=v.get) for k,v in word_counts.items()}\n",
    "\n",
    "mfc_model = MFCTagger(mfc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964f9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the missing value functionality\n",
    "\n",
    "def replace_unknown(sequence):\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44fbba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example decoding frequencies with MFC Tagger\n",
    "\n",
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5332a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "929a5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.72%\n",
      "testing accuracy mfc_model: 93.01%\n"
     ]
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798007e",
   "metadata": {},
   "source": [
    "## Hidden Markov Model tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a112f42",
   "metadata": {},
   "source": [
    "### Tag unigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4b05cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "\n",
    "    unigram_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    for inner_tuple in sequences:\n",
    "        for element in inner_tuple:\n",
    "            unigram_count[element] += 1\n",
    "        \n",
    "    return unigram_count\n",
    "\n",
    "tag_unigrams = unigram_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74588d9a",
   "metadata": {},
   "source": [
    "### Tag bigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e534358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(sequences):\n",
    "  \n",
    "    bigram_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    for inner_tuple in sequences:\n",
    "        for i in range(1, len(inner_tuple)):\n",
    "            bigram_count[(inner_tuple[i-1], inner_tuple[i])] += 1\n",
    "    \n",
    "    return bigram_count\n",
    "\n",
    "tag_bigrams = bigram_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c436ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6990e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c792f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
